{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /home/frederico/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay check\n"
     ]
    }
   ],
   "source": [
    "forum = webtext.raw('firefox.txt')\n",
    "print(forum[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.scripting.com/misc/msswitchad', 'http://www.watch.impress.co.jp', 'http://bugzilla.mozilla.org', 'http://www.http://mozilla.org', 'http://www.peterre.com', 'http://texturizer.net/firebird', 'http://foo', 'http://http://', 'http://james', 'http://www.lexis.com', 'http://www.woolworth.de', 'http://ftp.mozilla.org/pub/mozilla.org/firebird/nightly', 'http://http://', 'http://extensionroom.mozdev.org/more-info', 'http://www.odeon.co.uk/odeon', 'http://www.cctvusa.com', 'http://www.trenitalia.com/home/it', 'https://www.fortify.net', 'http://irc-galleria.net', 'http://http', 'http://www.mozilla.org/products', 'http://labs.google.com/cgi-bin', 'http://www.timbressuisses.ch', 'https://www.eposasp.com/ebpp', 'www.scripting.com/misc/msswitchad', 'www.foo.com', 'www.localhost.net.au', 'www.watch.impress.co.jp', 'www.*.com', 'www.aol.com', 'www.php.net', 'www.fnac.fr', 'www.http://mozilla.org', 'www.hvv.de', 'www.petetownshend.co.uk', 'www.google.com', 'www.wamu.com)', 'www.excite.com', 'www.peterre.com', 'www.logitech.com', 'www.mozilla.org', 'www.xy.com', 'www.blogger.com', 'www.pcpitstop.com', 'www.mozilla.org', 'www.zoneedit.com', 'www.libpr0n.com', 'www.us.army.mil', 'www.linuxmail.org', 'www.debian.org', 'www.lexis.com', 'www.lexis.com', 'www.m-w.com', 'www.woolworth.de', 'www.file.com', 'www.alternate.de)', 'www.microsoft.com', 'www.odeon.co.uk/odeon', 'www.cctvusa.com', 'www.mp3.de', 'www.domain', 'www.microsoft.com', 'www.trenitalia.com/home/it', 'www.rmvplus.de', 'www.fortify.net', 'www.microsoft.com', \"www.uboot.com'\", 'www.microsoft.com', 'www.mozilla.org/products', 'www.lycos.co.uk', 'www.calciomercato.com', 'www.odeon.co.uk/odeon', 'www.atozwebtools.com', 'www.X.com', 'www.timbressuisses.ch', 'www.vipernetworks.com', 'www.eposasp.com/ebpp', 'www.yahoo.com', 'www.intellicast.com', 'www.w3c.org']\n",
      "['Ctrl+M', 'ctrl+t', 'Ctrl+M', 'ALT+F', 'ctrl+d', 'Ctrl+C', 'ctrl+C', 'CTRL+c', 'Ctrl+E', 'Ctrl+B', 'ctrl + e', 'Ctrl+E', 'CTRL+F', 'Ctrl+T', 'Ctrl+S', 'Ctrl+S', 'Alt+H', 'ctrl+d', 'ctrl+t', 'Alt + D', 'shift+l', 'ctrl + s', 'CTRL + m', 'ctrl+e', 'alt+e', 'ctrl+e', 'ctrl+e', 'Ctrl+-', 'Ctrl++', 'ctrl+e', 'ALT + L', 'Ctrl+L', 'Shift+C', 'ctrl+e', 'Ctrl+Q', 'Ctrl+W', 'alt+b', 'Ctrl+L', 'alt+f', 'Ctrl+L', 'alt+s', 'shift+s', 'shift+s', 'alt+s', 'Ctrl+E', 'ALT+D', 'Alt+D', 'Alt+d', 'Ctrl+E', 'CTRL+E', 'Shift+G', 'ctrl+p', 'Alt+E', 'ctrl +\\r\\nA', 'ALT+d', 'Ctrl+S', 'Alt+C', 'ctrl++', 'Shift +F', 'Alt+F', 'Alt+d', 'CTRL+K', 'Ctrl+W', 'Ctrl+W', 'Shift+E', 'Alt+E', 'Ctrl+M', 'Ctrl+K', 'Ctrl+T', 'CTRL+E', 'alt+e', 'alt+e', 'Ctrl+W', 'Ctrl + V', 'Shift + V', 'ctrl+T', 'Ctrl+P', 'Alt+E', 'Ctrl + u', 'Shift+C', 'Alt+E', 'Ctrl+T', 'CTRL+Y', 'CTRL+L', 'Ctrl+K', 'Ctrl+K', 'Alt+D', 'Ctrl + B', 'Alt + S', 'Ctrl+A', 'Ctrl+F', 'Ctrl+T', 'Ctrl+E', 'Ctrl+W', 'Ctrl+t', 'Ctrl+x', 'Alt+f', 'Ctrl+ C', 'ctrl+K', 'ctrl+K', 'Alt+D', 'Ctrl+E', 'Ctrl+K', 'Ctrl+S', 'Ctrl+m', 'Ctrl+E', 'Ctrl+w', 'Shift+c', 'ctrl+p', 'Alt+f', 'ctrl++', 'SHIFT + T', 'Alt+E', 'Ctrl+P', 'Ctrl+K', 'Shift+D', 'Alt+D', 'shift+d', 'ctrl+f', 'Ctrl+W', 'ctrl+t', 'Alt+L', 'Ctrl+m', 'ALT+F', 'Ctrl+B', 'Ctrl+B', 'CTRL+B', 'CTRL+B', 'CTRL+I', 'Ctrl+B', 'Ctrl + K', 'Ctrl+S', 'CTRL+P', 'Alt+f', 'Ctrl+R', 'CTRL+Y', 'CTRL+E', 'Ctrl+E', 'Ctrl+E', 'Ctrl+E', 'CTRL+T', 'CTRL+A', 'Ctrl+T', 'Ctrl+S', 'Ctrl + B', 'Ctrl+E', 'ctrl+e', 'Ctrl+F', 'Ctrl+F', 'CTRL + F', 'CTRL + F']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "http_urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', forum)\n",
    "www_urls = re.findall('www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', forum)\n",
    "#regex = '(?:https?:\\/\\/)?(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}'\n",
    "#urls = re.findall(regex, forum)\n",
    "urls = http_urls + www_urls\n",
    "shortcuts = re.findall('(?:ctrl|shift|alt)+\\s*\\+\\s*[a-z+\\-.,/]', forum, flags=re.IGNORECASE)\n",
    "print(urls)\n",
    "print(shortcuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/frederico/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo a\n"
     ]
    }
   ],
   "source": [
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "print(hamlet[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 classes tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "PATH_TO_JAR = '/home/frederico/Desktop/DS/stanford-ner-2020-11-17/stanford-ner.jar'\n",
    "PATH_TO_MODEL = '/home/frederico/Desktop/DS/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "NER = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Voltemand', 'Horatio', 'Sallets', 'Carbuncles', 'Controuersie', 'Vnckle', 'Scullion', 'Fox', 'Throate', 'Pesant']\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "words = nltk.wordpunct_tokenize(hamlet)\n",
    "tagged = NER.tag(words)\n",
    "people = []\n",
    "\n",
    "for (word,label) in tagged:\n",
    "    if label == 'PERSON':\n",
    "        people.append(word)\n",
    "people = list(set(people))\n",
    "print(people[:10])\n",
    "print(len(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 classes tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Voltemand', 'Horatio', 'Sallets', 'Carbuncles', 'Controuersie', 'Vnckle', 'Scullion', 'Fox', 'Throate', 'Pesant']\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_JAR = '/home/frederico/Desktop/DS/stanford-ner-2020-11-17/stanford-ner.jar'\n",
    "PATH_TO_MODEL = '/home/frederico/Desktop/DS/stanford-ner-2020-11-17/classifiers/english.conll.4class.distsim.crf.ser.gz'\n",
    "NER = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
    "\n",
    "tagged = NER.tag(words)\n",
    "people = []\n",
    "\n",
    "for (word,label) in tagged:\n",
    "    if label == 'PERSON':\n",
    "        people.append(word)\n",
    "people = list(set(people))\n",
    "print(people[:10])\n",
    "print(len(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 classes tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaue', 'Sonne', 'Horatio', 'Keepes', 'Barnardo', 'Gertrude', 'Businesse', 'Scullion', 'Leuies', 'Scul']\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_JAR = '/home/frederico/Desktop/DS/stanford-ner-2020-11-17/stanford-ner.jar'\n",
    "PATH_TO_MODEL = '/home/frederico/Desktop/DS/stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.crf.ser.gz'\n",
    "NER = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
    "\n",
    "tagged = NER.tag(words)\n",
    "people = []\n",
    "\n",
    "for (word,label) in tagged:\n",
    "    if label == 'PERSON':\n",
    "        people.append(word)\n",
    "people = list(set(people))\n",
    "print(people[:10])\n",
    "print(len(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignement 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/frederico/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Ja\n"
     ]
    }
   ],
   "source": [
    "rts = reuters.raw('test/14826')\n",
    "print(rts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUMITOMO', 'BANK', 'AIMS', 'AT', 'QUICK', 'RECOVERY', 'FROM', 'MERGER', 'Sumitomo', 'Bank']\n",
      "acq\n"
     ]
    }
   ],
   "source": [
    "documents = [(list(reuters.words(fileid)), category)\n",
    "             for category in reuters.categories()\n",
    "             for fileid in reuters.fileids(category)]\n",
    "\n",
    "# first 10 words of 1 document\n",
    "print(documents[0][0][:10])\n",
    "# category of 1 document\n",
    "print(documents[0][1])\n",
    "\n",
    "import random\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most common words among all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 94687), (',', 72360), ('the', 69277), ('of', 36779), ('to', 36400)]\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in reuters.words())\n",
    "# 5 most common words\n",
    "print(all_words.most_common(5))\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# check if the document has the most common words among it's words\n",
    "def document_features(document):\n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "print(len(featuresets))\n",
    "train_set, test_set = featuresets[5000:], featuresets[:200]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gas\n",
      "['trade']\n",
      "Most Informative Features\n",
      "          contains(palm) = True           palm-o : earn   =   1614.4 : 1.0\n",
      "        contains(rubber) = True           rubber : earn   =   1433.8 : 1.0\n",
      "          contains(zinc) = True             zinc : earn   =   1396.7 : 1.0\n",
      "      contains(supplies) = True           propan : earn   =   1372.8 : 1.0\n",
      "        contains(coffee) = True           coffee : earn   =   1355.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(classifier.classify(document_features(rts)))\n",
    "print(reuters.categories('test/14826'))\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most common words among the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_document_features(document):\n",
    "    document_words = nltk.FreqDist(w.lower() for w in document)\n",
    "    word_features = list(document_words)[:2000]\n",
    "    return dict([(word, True) for word in word_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13328\n",
      "({'the': True, ',': True, 'stock': True, '.': True, '-': True, 'dividend': True, 'on': True, 'hydraulic': True, 'said': True, 'split': True, 'of': True, 'will': True, 'april': True, 'share': True, '3': True, 'its': True, 'a': True, 'common': True, 'quarterly': True, 'cash': True, 'to': True, 'be': True, 'cts': True, 'per': True, 'for': True, 'it': True, '50': True, \"'\": True, 's': True, 'payable': True, 'stockholders': True, 'record': True, 'that': True, 'outstanding': True, 'company': True, '&': True, 'lt': True, ';': True, 'thc': True, '>': True, 'splits': True, '2': True, 'hikes': True, 'co': True, 'board': True, 'approved': True, 'three': True, 'two': True, 'and': True, 'increased': True, 'occur': True, 'through': True, 'pct': True, 'distribution': True, '30': True, '15': True, 'is': True, 'paid': True, 'pre': True, 'shares': True, 'are': True, 'currently': True, '54': True, '75': True, 'up': True, 'from': True, '52': True, 'represent': True, '36': True, 'after': True}, 'earn')\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(alternative_document_features(d), c) for (d,c) in documents]\n",
    "print(len(featuresets))\n",
    "print(featuresets[0])\n",
    "train_set, test_set = featuresets[5000:], featuresets[:200]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfl\n",
      "['trade']\n",
      "Most Informative Features\n",
      "                  coffee = True           coffee : earn   =   1622.3 : 1.0\n",
      "                 propane = True           propan : earn   =   1468.8 : 1.0\n",
      "                    oats = True              oat : earn   =   1428.0 : 1.0\n",
      "               argentine = True           lin-oi : earn   =   1360.0 : 1.0\n",
      "                minister = True            nzdlr : earn   =   1360.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(classifier.classify(alternative_document_features(rts)))\n",
    "print(reuters.categories('test/14826'))\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the difference in results between both extractors, we can conclude that the first method of extracting information, that is, extracting the most common words across all texts and then check which of them are present for a given file, has a very supperior accuracy to the normal bag of words approach (54% accuracy vs 0.5% accuracy).\n",
    "One possible reason for this might be that having a basis of possible words (the most common across all files) allows for a easier training of the model and for an easier recognition of the class of the file since we can focus on a smaller and more focused set of words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e511838c1e57aaa348af65028b1d0e807fc0ba78c128e77bcd38a63295926b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
